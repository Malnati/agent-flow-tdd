# Prompt TDD

Um sistema para desenvolvimento orientado a testes usando prompts de IA.

## üöÄ Funcionalidades

- Gera√ß√£o de features com crit√©rios de aceite e cen√°rios de teste
- An√°lise de complexidade e estimativas
- Suporte a m√∫ltiplos modelos de IA (GPT-3.5, GPT-4)
- Interface CLI com modo interativo e MCP (Multi-Command Protocol)
- Sa√≠da em formatos JSON e Markdown
- Configura√ß√£o unificada e centralizada

## üìã Pr√©-requisitos

- Python 3.13+
- Chave de API OpenAI (`OPENAI_API_KEY`)
- Ambiente virtual Python (venv)

## üõ†Ô∏è Instala√ß√£o

1. Clone o reposit√≥rio:
```bash
git clone https://github.com/seu-usuario/prompt-tdd.git
cd prompt-tdd
```

2. Configure as vari√°veis de ambiente necess√°rias:
```bash
# N√£o use arquivos .env, configure diretamente no ambiente
export OPENAI_API_KEY="sua-chave-aqui"
```

3. Instale as depend√™ncias:
```bash
make install
```

## ‚öôÔ∏è Configura√ß√£o

O projeto usa um arquivo de configura√ß√£o unificado em `src/configs/cli.yaml` com tr√™s se√ß√µes principais:

### 1. CLI (`cli`)
- Configura√ß√µes de sa√≠da (formatos, indenta√ß√£o)
- Configura√ß√µes do modelo (nome, temperatura)
- Mensagens do sistema
- Formata√ß√£o JSON

### 2. MCP (`mcp`)
- Configura√ß√µes de logging do MCP
- Configura√ß√µes do LLM
- Configura√ß√µes do handler MCP
- Formatos de metadados

### 3. Aplica√ß√£o (`app`)
- Configura√ß√µes do modelo
- Configura√ß√µes do banco de dados
- Configura√ß√µes de logging
- Configura√ß√µes de exemplo
- Configura√ß√µes de resultado

## üéÆ Comandos Dispon√≠veis

### `make install`
Instala todas as depend√™ncias do projeto.

```bash
make install
```

### `make test`
Executa todos os testes do projeto.

```bash
make test
```

### `make cli`
Inicia o CLI para processamento de features.

```bash
make cli

# Exemplos de uso:
prompt-tdd feature "Criar sistema de login com autentica√ß√£o de dois fatores"
prompt-tdd feature "Criar sistema de cadastro de usu√°rios" --model gpt-4-turbo
prompt-tdd feature "Criar API REST" --format markdown
prompt-tdd status
```

#### Op√ß√µes do comando `feature`:
- `--model, -m`: Modelo a ser usado (default: gpt-3.5-turbo)
- `--elevation-model, -e`: Modelo para fallback (default: gpt-4-turbo)
- `--force, -f`: For√ßa uso do modelo sem fallback
- `--api-key, -k`: Chave da API (opcional)
- `--timeout, -t`: Tempo limite em segundos (default: 30)
- `--max-retries, -r`: M√°ximo de tentativas (default: 3)
- `--temperature, -temp`: Temperatura do modelo (default: 0.7)
- `--max-tokens, -mt`: Limite de tokens (opcional)
- `--format, -fmt`: Formato de sa√≠da (json/markdown)

### Protocolo MCP (Model Context Protocol)

O projeto agora suporta o [Model Context Protocol](https://github.com/modelcontextprotocol/protocol) oficial, permitindo:
- Integra√ß√£o padronizada com diferentes modelos de IA
- Comunica√ß√£o bidirecional via protocolo MCP
- Suporte a streaming e eventos ass√≠ncronos

#### Como Funciona

1. Inicie o modo MCP:
```bash
prompt-tdd mcp
```

2. Envie mensagens no formato MCP:
```json
{
  "content": "Criar sistema de login",
  "metadata": {
    "type": "feature",
    "options": {
      "model": "gpt-4-turbo",
      "temperature": 0.7,
      "format": "json"
    }
  }
}
```

3. Receba respostas no formato MCP:
```json
{
  "content": {
    "feature": "Sistema de Login",
    "acceptance_criteria": [...],
    "test_scenarios": [...],
    "complexity": 3
  },
  "metadata": {
    "status": "success",
    "type": "feature"
  }
}
```

## ü§ñ Integra√ß√£o de Modelos

O projeto usa o Model Context Protocol para integra√ß√£o com diferentes modelos:

### 1. Via SDK MCP

```python
from src.core.agents import AgentOrchestrator
from src.core.models import ModelManager
from src.core.db import DatabaseManager

# Inicializa componentes
model_manager = ModelManager()
db = DatabaseManager()

# Cria orquestrador
orchestrator = AgentOrchestrator(model_manager, db)

# Executa
result = orchestrator.execute(
    prompt="Criar sistema de login",
    session_id="exemplo",
    format="json"
)

# Processa resultado
print(f"Sa√≠da: {result.output}")
print(f"Items: {len(result.items)}")
print(f"Guardrails: {len(result.guardrails)}")
print(f"Respostas: {len(result.raw_responses)}")
```

### 2. Via CLI

```bash
# OpenAI GPT-4
prompt-tdd feature "Criar API" --model gpt-4-turbo --api-key $OPENAI_API_KEY

# Anthropic Claude
prompt-tdd feature "Criar API" --model claude-3 --api-key $ANTHROPIC_KEY
```

### 3. Via MCP

Especifique o modelo nas options:

```json
{
  "content": "Criar API REST",
  "metadata": {
    "type": "feature",
    "options": {
      "model": "gpt-4-turbo",
      "api_key": "sua-chave",
      "temperature": 0.7
    }
  }
}
```

### Modelos Suportados

O sistema suporta v√°rios modelos de IA que podem ser especificados no par√¢metro `model`:

#### Modelos Locais:
- `tinyllama-1.1b` - Modelo local TinyLLaMA, leve e r√°pido
- `phi-1` - Modelo local Phi-1 da Microsoft
- `deepseek-coder-6.7b` - Modelo local DeepSeek Coder especializado em c√≥digo
- `phi3-mini` - Modelo local Phi-3 Mini da Microsoft

#### Modelos Remotos:
- `gpt-3.5-turbo` - OpenAI GPT-3.5 Turbo (requer chave OpenAI)
- `gpt-4` - OpenAI GPT-4 (requer chave OpenAI)
- `gemini-pro` - Google Gemini Pro (requer chave Gemini)
- `claude-3-opus` - Anthropic Claude 3 Opus (requer chave Anthropic)

### Exemplo de uso com modelo espec√≠fico:

```bash
# Usando modelo local
make tdd prompt="Cadastro de pessoas" format=json model="deepseek-coder-6.7b"

# Usando modelo remoto
make tdd prompt="Cadastro de pessoas" format=markdown model="gpt-4"
```

### Configura√ß√£o de Modelos Adicionais

Cada modelo pode ser baixado separadamente:

```bash
# Baixar TinyLLaMA
make download-model

# Baixar Phi-1
make download-phi1

# Baixar DeepSeek Coder
make download-deepseek

# Baixar Phi-3 Mini
make download-phi3
```

## üß™ Testes

O projeto usa pytest para testes. Execute:

```bash
make test
```

### Testes Opcionais

Alguns testes s√£o desabilitados por padr√£o por serem muito lentos. Para execut√°-los:

```bash
# Executa testes de instala√ß√£o
pytest -v -m "install" src/tests/test_e2e.py
```

## üìù Logs

Os logs s√£o gerados automaticamente com:
- N√≠vel INFO para entrada/sa√≠da de fun√ß√µes
- N√≠vel DEBUG para estados intermedi√°rios
- N√≠vel ERROR para exce√ß√µes (com stacktrace)
- Configura√ß√£o centralizada em `cli.yaml`

## üîí Vari√°veis de Ambiente

Vari√°veis obrigat√≥rias:
- `OPENAI_API_KEY`: Chave da API OpenAI

Vari√°veis opcionais:
- `OPENROUTER_KEY`: Chave da API OpenRouter
- `GEMINI_KEY`: Chave da API Gemini
- `ANTHROPIC_KEY`: Chave da API Anthropic
- `LOG_LEVEL`: N√≠vel de log (default: INFO)
- `CACHE_DIR`: Diret√≥rio de cache
- `CACHE_TTL`: Tempo de vida do cache
- `FALLBACK_ENABLED`: Habilita fallback de modelos
- `DEFAULT_MODEL`: Modelo padr√£o
- `ELEVATION_MODEL`: Modelo para fallback
- `MODEL_TIMEOUT`: Timeout para chamadas de modelo
- `MAX_RETRIES`: M√°ximo de tentativas

**IMPORTANTE**: N√£o use arquivos .env. Configure as vari√°veis diretamente no ambiente ou via argumentos.

## ü§ù Contribuindo

1. Fork o projeto
2. Crie sua branch de feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## Uso

O Prompt TDD pode ser usado de duas formas:

### 1. Usando o Makefile (recomendado)

```bash
# Criar uma nova feature
make run prompt-tdd="Criar um sistema de login com autentica√ß√£o JWT"

# Verificar status do ambiente
make run prompt-tdd="" mode=status

# Iniciar servidor MCP em background (sem sa√≠da no terminal)
make run prompt-tdd="" mode=mcp
# O servidor MCP ser√° iniciado em background e voc√™ ver√° apenas o PID do processo
```

### 2. Usando o comando diretamente

Primeiro, ative o ambiente virtual:

```bash
source .venv/bin/activate
```

Ent√£o use o comando `prompt-tdd`:

```bash
# Criar uma nova feature
prompt-tdd "Criar um sistema de login com autentica√ß√£o JWT"

# Verificar status do ambiente
prompt-tdd --mode status ""

# Iniciar servidor MCP (ir√° bloquear o terminal e mostrar logs)
prompt-tdd --mode mcp ""

# Ou inicie em background sem logs
nohup prompt-tdd --mode mcp "" > /dev/null 2>&1 &
```

## Op√ß√µes

- `--format`: Formato de sa√≠da (json ou markdown). Padr√£o: json
- `--mode`: Modo de opera√ß√£o (feature, status ou mcp). Padr√£o: feature

## Testes

```bash
make test
```

## Limpeza

```bash
make clean
```

# Agent Flow TDD

<p align="center">
  <img src="assets/logo.png" alt="Agent Flow TDD Logo" width="400">
</p>

Framework para desenvolvimento orientado a testes usando agentes de IA.

## üöÄ Vis√£o Geral

O Agent Flow TDD √© um framework que utiliza agentes de IA para auxiliar no desenvolvimento orientado a testes (TDD). Ele fornece uma estrutura para criar, testar e implantar aplica√ß√µes usando prompts de IA.

## üìã Principais Funcionalidades

- **Desenvolvimento orientado a testes** para agentes de IA
- **M√∫ltiplos modelos** suportados (locais e remotos)
- **Interface CLI** com modo interativo
- **Protocolo MCP** (Model Context Protocol) para integra√ß√£o padronizada
- **Guardrails** para valida√ß√£o de entradas e sa√≠das
- **Logging estruturado** em SQLite

## üöÄ In√≠cio R√°pido

```bash
# Clone o reposit√≥rio
git clone https://github.com/Malnati/agent-flow-tdd.git
cd agent-flow-tdd

# Instale as depend√™ncias
make install

# Execute o comando principal
make tdd prompt="Criar API REST" format=json
```

## üìñ Documenta√ß√£o

A documenta√ß√£o completa est√° dispon√≠vel no diret√≥rio [docs/](docs/):

- [Vis√£o Geral](docs/overview/README.md) - Objetivos, arquitetura e tecnologias
- [Instala√ß√£o](docs/installation/README.md) - Como instalar e configurar
- [Uso](docs/usage/README.md) - Como usar a CLI e o modo MCP
- [Banco de Dados](docs/database/README.md) - Estrutura e gerenciamento do banco de dados
- [Logs](docs/logs/README.md) - Sistema de logging e monitoramento
- [Troubleshooting](docs/troubleshooting/README.md) - Resolu√ß√£o de problemas comuns

## üíª Modelos Suportados

O sistema suporta diversos modelos de IA que podem ser utilizados atrav√©s do par√¢metro `model`:

### Modelos Locais
- `tinyllama-1.1b` - Modelo local TinyLLaMA (1.1B)
- `phi1` - Modelo local Phi-1 da Microsoft (1.3B)
- `phi2` - Modelo local Phi-2 da Microsoft (2.7B)
- `deepseek_local` - Modelo local DeepSeek Coder (6.7B)
- `phi3` - Modelo local Phi-3 Mini da Microsoft

### Modelos Remotos (API)
- `gpt-3.5-turbo` e `gpt-4-turbo` - OpenAI
- `claude-3-opus`, `claude-3-sonnet` - Anthropic
- `gemini-pro` - Google

## üõ†Ô∏è Comandos Principais

```bash
# Executar o comando principal
make tdd prompt="Criar um sistema de login" format=json

# Especificar um modelo
make tdd prompt="Criar API REST" model=deepseek_local

# Verificar status do ambiente
make status

# Executar testes
make test

# Visualizar logs
make logs
```

## ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Consulte [docs/development/README.md](docs/development/README.md) para instru√ß√µes sobre como contribuir para o projeto.

## üìù Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT - veja o arquivo LICENSE para mais detalhes.

## üê≥ Usando com Docker

### Pr√©-requisitos
- Docker
- Docker Compose

### Configura√ß√£o
1. Copie o arquivo de exemplo de vari√°veis de ambiente:
```bash
cp .docker/.env.example .docker/.env
```

2. Configure suas chaves de API no arquivo `.docker/.env`:
```env
# Chaves de API
OPENAI_API_KEY=sua-chave-openai
ANTHROPIC_KEY=sua-chave-anthropic
GEMINI_KEY=sua-chave-gemini

# Configura√ß√µes do modelo
DEFAULT_MODEL=gpt-3.5-turbo
ELEVATION_MODEL=gpt-4-turbo

# Configura√ß√µes do banco
DB_PATH=/app/data/database.db
DB_HISTORY_LIMIT=100

# Configura√ß√µes de logging
LOG_LEVEL=INFO
LOG_FORMAT=json
```

### Executando
Para desenvolvimento:
```bash
docker-compose -f .docker/docker-compose.yml run dev
```

Para produ√ß√£o:
```bash
docker-compose -f .docker/docker-compose.yml run app
```

### Exemplo de uso com Docker
```python
from src.core.agents import AgentOrchestrator
from src.core.models import ModelManager
from src.core.db import DatabaseManager

# Inicializa componentes
model_manager = ModelManager()
db = DatabaseManager()

# Cria orquestrador
orchestrator = AgentOrchestrator(model_manager, db)

# Executa
result = orchestrator.execute(
    prompt="Criar sistema de login",
    session_id="docker-exemplo",
    format="json"
)

# Processa resultado
print(f"Sa√≠da: {result.output}")
print(f"Items: {len(result.items)}")
print(f"Guardrails: {len(result.guardrails)}")
print(f"Respostas: {len(result.raw_responses)}")
```

### Comandos √öteis
- Construir as imagens:
```bash
docker-compose -f .docker/docker-compose.yml build
```

- Visualizar logs:
```bash
docker-compose -f .docker/docker-compose.yml logs -f
```

- Limpar volumes:
```bash
docker-compose -f .docker/docker-compose.yml down -v
```