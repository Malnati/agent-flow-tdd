models:
  defaults:
    model: deepseek-local-coder                    # Modelo principal padrão
    elevation_model: phi3-mini      # Modelo de fallback se o principal falhar
    temperature: 0.7                # Temperatura padrão para geração
    max_tokens: 1024                # Máximo de tokens gerados por resposta
    timeout: 15                     # Tempo limite de resposta do modelo (segundos)
    max_retries: 2                  # Quantidade de tentativas em caso de erro

  env_vars:
    default_model: DEFAULT_MODEL              # Variável de ambiente para modelo padrão
    elevation_model: ELEVATION_MODEL          # Variável de ambiente para fallback
    openai_key: OPENAI_API_KEY                # Chave de API para OpenAI
    openrouter_key: OPENROUTER_API_KEY        # Chave de API para OpenRouter
    gemini_key: GEMINI_API_KEY                # Chave de API para Gemini
    anthropic_key: ANTHROPIC_API_KEY          # Chave de API para Anthropic
    model_timeout: MODEL_TIMEOUT              # Timeout customizado por env
    max_retries: MAX_RETRIES                  # Tentativas máximas por env
    fallback_enabled: MODEL_FALLBACK_ENABLED  # Ativação de fallback por env
    cache_enabled: MODEL_CACHE_ENABLED        # Ativação de cache por env
    cache_ttl: MODEL_CACHE_TTL                # TTL de cache por env

  fallback:
    enabled: true  # Ativa ou desativa uso de fallback automático

  cache:
    enabled: true  # Ativa cache de respostas
    ttl: 300       # Tempo de validade das respostas em cache (segundos)

  providers:
    - name: openai-gpt-3.5-turbo                          # Provedor OpenAI via API oficial
      prefix_pattern: gpt-3.5-turbo
      model_path: "./models/gpt-3.5-turbo.gguf"
      dir: "./models/openai"
      n_ctx: 2048
      n_threads: 4
      model: gpt-3.5-turbo
      url: "https://api.openai.com/v1/models/gpt-3.5-turbo"

    - name: openai-gpt-4                          # Provedor OpenAI via API oficial
      prefix_pattern: gpt-4
      model_path: "./models/gpt-4.gguf"
      dir: "./models/openai"
      n_ctx: 2048
      n_threads: 4
      model: gpt-4
      url: "https://api.openai.com/v1/models/gpt-4"

    - name: openrouter-meta-llama-3-8b                      # Provedor OpenRouter via API
      prefix_pattern: meta-llama/llama-3-8b
      model_path: "./models/meta-llama-3-8b.gguf"
      dir: "./models/openrouter"
      n_ctx: 2048
      n_threads: 4
      model: meta-llama-3-8b
      url: "https://openrouter.ai/api/v1/models/meta-llama/llama-3-8b"

    - name: openrouter-deepseek-coder-7b-instruct-q4                      # Provedor OpenRouter via API
      prefix_pattern: deepseek-coder:7b-instruct-q4
      model_path: "./models/deepseek-coder-7b-instruct-q4.gguf"
      dir: "./models/openrouter"
      n_ctx: 2048
      n_threads: 4
      model: deepseek-coder-7b-instruct-q4
      url: "https://openrouter.ai/api/v1/models/deepseek-coder:7b-instruct-q4"

    - name: gemini-pro                          # Provedor Gemini
      prefix_pattern: gemini-pro
      model_path: "./models/gemini-pro.gguf"
      dir: "./models/gemini"
      n_ctx: 2048
      n_threads: 4
      model: gemini-pro
      url: "https://gemini.api/models/gemini-pro"

    - name: anthropic-claude-3-opus-20240229                       # Provedor Anthropic
      prefix_pattern: claude-3-opus-20240229
      model_path: "./models/claude-3-opus-20240229.gguf"
      dir: "./models/anthropic"
      n_ctx: 2048
      n_threads: 4
      model: claude-3-opus-20240229
      url: "https://anthropic.api/models/claude-3-opus-20240229"

    - name: tinyllama-1.1b                       # Provedor TinyLlama
      prefix_pattern: tinyllama-1.1b
      model_path: "./models/tinyllama-1.1b.gguf"
      dir: "./models/tinyllama"
      n_ctx: 2048
      n_threads: 4
      model: tinyllama-1.1b
      url: "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"

    - name: phi1                            # Modelo local executado via llama.cpp
      prefix_pattern: phi-1
      model_path: "./models/phi-1.gguf"
      dir: "./models/phi"
      n_ctx: 2048
      n_threads: 4
      model: phi-1
      url: "https://huggingface.co/professorf/phi-1-gguf/resolve/main/phi-1-f16.gguf"

    - name: deepseek-local-coder                  # Provedor DeepSeek local
      prefix_pattern: deepseek-local-coder
      model_path: "./models/deepseek-coder.gguf"
      dir: "./models/deepseek"
      n_ctx: 2048
      n_threads: 4
      model: deepseek-local-coder
      url: "https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q4_K_M.gguf"

    - name: deepseek-coder-awq                  # Provedor DeepSeek local
      prefix_pattern: deepseek-coder-awq
      model_path: "./models/deepseek-coder-awq.gguf"
      dir: "./models/deepseek"
      n_ctx: 2048
      n_threads: 4
      model: deepseek-coder-awq
      url: "https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-AWQ/tree/main"

    - name: phi3-mini                            # Modelo local executado via llama.cpp
      prefix_pattern: phi3-mini
      model_path: "./models/phi3-mini.gguf"
      dir: "./models/phi3"
      n_ctx: 2048
      n_threads: 4
      model: phi3-mini
      url: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/raw/main/Phi-3-mini-4k-instruct-q4.gguf"

    - name: phi3-mini-fp16                            # Modelo local executado via llama.cpp
      prefix_pattern: phi3-mini-fp16
      model_path: "./models/phi3-mini.gguf"
      dir: "./models/phi3"
      n_ctx: 2048
      n_threads: 4
      model: phi3-mini-fp16
      url: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/raw/main/Phi-3-mini-4k-instruct-fp16.gguf"