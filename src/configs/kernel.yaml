models:
  defaults:
    model: deepseek-local-coder                    # Modelo principal padrão
    elevation_model: phi3-mini      # Modelo de fallback se o principal falhar
    temperature: 0.7                # Temperatura padrão para geração
    max_tokens: 1024                # Máximo de tokens gerados por resposta
    timeout: 15                     # Tempo limite de resposta do modelo (segundos)
    max_retries: 2                  # Quantidade de tentativas em caso de erro

  env_vars:
    default_model: DEFAULT_MODEL              # Variável de ambiente para modelo padrão
    elevation_model: ELEVATION_MODEL          # Variável de ambiente para fallback
    openai_key: OPENAI_API_KEY                # Chave de API para OpenAI
    openrouter_key: OPENROUTER_API_KEY        # Chave de API para OpenRouter
    gemini_key: GEMINI_API_KEY                # Chave de API para Gemini
    anthropic_key: ANTHROPIC_API_KEY          # Chave de API para Anthropic
    model_timeout: MODEL_TIMEOUT              # Timeout customizado por env
    max_retries: MAX_RETRIES                  # Tentativas máximas por env
    fallback_enabled: MODEL_FALLBACK_ENABLED  # Ativação de fallback por env
    cache_enabled: MODEL_CACHE_ENABLED        # Ativação de cache por env
    cache_ttl: MODEL_CACHE_TTL                # TTL de cache por env

  fallback:
    enabled: true  # Ativa ou desativa uso de fallback automático

  cache:
    enabled: true  # Ativa cache de respostas
    ttl: 300       # Tempo de validade das respostas em cache (segundos)

  providers:
    - name: openai                          # Provedor OpenAI via API oficial
      prefix_patterns: ["gpt-", "text-"]
      models: ["gpt-3.5-turbo", "gpt-4"]
      url: "https://api.openai.com/v1/models"

    - name: openrouter                      # Provedor OpenRouter via API
      prefix_patterns: ["meta-llama/", "mistral/", "deepseek-coder:"]
      base_url: "https://openrouter.ai/api/v1"
      models: ["meta-llama/llama-3-8b", "deepseek-coder:7b-instruct-q4"]
      url: "https://openrouter.ai/api/v1/models"

    - name: gemini                          # Provedor Gemini
      prefix_patterns: ["gemini-"]
      default_model: "gemini-pro"
      models: ["gemini-pro"]
      url: "https://gemini.api/models"

    - name: anthropic                       # Provedor Anthropic
      prefix_patterns: ["claude-"]
      models: ["claude-3-opus-20240229"]
      url: "https://anthropic.api/models"

    - name: tinyllama                       # Provedor TinyLlama
      prefix_patterns: ["tinyllama-"]
      model_path: "./models/tinyllama-1.1b.gguf"
      n_ctx: 2048
      n_threads: 4
      models: ["tinyllama-1.1b"]
      url: "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"

    - name: phi1                            # Modelo local executado via llama.cpp
      prefix_patterns: ["phi-1"]
      model_path: "./models/phi-1.gguf"
      n_ctx: 2048
      n_threads: 4
      models: ["phi-1"]
      url: "https://huggingface.co/professorf/phi-1-gguf/resolve/main/phi-1-f16.gguf"

    - name: deepseek_local                  # Provedor DeepSeek local
      prefix_patterns: ["deepseek-local-"]
      model_path: "./models/deepseek-coder.gguf"
      n_ctx: 2048
      n_threads: 4
      models: ["deepseek-local-coder"]
      url: "https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q4_K_M.gguf"

    - name: phi3                            # Modelo local executado via llama.cpp
      prefix_patterns: ["phi3-"]
      model_path: "./models/phi3-mini.gguf"
      n_ctx: 2048
      n_threads: 4
      models: ["phi3-mini", "phi3-mini-fp16"]
      url: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/raw/main/Phi-3-mini-4k-instruct-q4.gguf"
      url_fp16: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/raw/main/Phi-3-mini-4k-instruct-fp16.gguf"